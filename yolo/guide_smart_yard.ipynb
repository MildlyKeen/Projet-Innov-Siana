{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d37d5837",
   "metadata": {},
   "source": [
    "\n",
    "# Guide de prise en main du projet **Smart Yard**\n",
    "\n",
    "Ce notebook pédagogique présente la structure du dépôt et décrit comment utiliser les scripts fournis pour entraîner, valider et exploiter les modèles de détection et de segmentation utilisés dans le projet Smart Yard.  \n",
    "Il est conçu comme un document de prise en main pour un nouveau développeur ou un évaluateur technique.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb250ef",
   "metadata": {},
   "source": [
    "\n",
    "## Sommaire\n",
    "\n",
    "1. [Structure du projet](#Structure)\n",
    "2. [Rôle de chaque dossier et fichiers clés](#Roles)\n",
    "3. [Préparation des données et entraînement des modèles](#Entrainement)\n",
    "   - [Détection des trains](#Detection)\n",
    "   - [Segmentation des voies](#Segmentation)\n",
    "4. [Validation et tests](#Validation)\n",
    "5. [Scripts d’inférence](#Inference)\n",
    "   - [Script d’inférence des voies (rails)](#InfRails)\n",
    "   - [Script d’inférence des trains](#InfTrains)\n",
    "   - [Script combiné trains + rails](#InfCombi)\n",
    "6. [Historique d’occupation des voies](#Historique)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4625d243",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Structure du projet {#Structure}\n",
    "\n",
    "L’arborescence du dépôt est organisée pour séparer clairement les données brutes, les jeux de données préparés, les configurations, les entraînements, les modèles et les scripts d’inférence. Ci‑dessous, un extrait de l’arborescence principale avec les dossiers clés.  \n",
    "Le code suivant parcourt le répertoire racine (`/home/oai/share` dans le dépôt fourni) et affiche les premiers niveaux de dossiers pour donner un aperçu compact.  \n",
    "\n",
    "*Exécutez la cellule de code si vous travaillez dans un environnement Jupyter pour obtenir la vue actualisée ; sinon, reportez‑vous à l’extrait textuel ci‑dessous.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4fb88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "def print_tree(path, prefix='', depth=0, max_depth=2):\n",
    "    # Affiche l'arborescence limitée à max_depth niveaux.\n",
    "    if depth > max_depth:\n",
    "        return\n",
    "    try:\n",
    "        items = sorted(os.listdir(path))\n",
    "    except Exception as e:\n",
    "        print(f\"[Erreur lors du listing de {path}: {e}]\")\n",
    "        return\n",
    "    for i, name in enumerate(items):\n",
    "        branch = '└── ' if i == len(items) - 1 else '├── '\n",
    "        print(prefix + branch + name)\n",
    "        new_path = os.path.join(path, name)\n",
    "        if os.path.isdir(new_path):\n",
    "            new_prefix = prefix + ('    ' if i == len(items) - 1 else '│   ')\n",
    "            print_tree(new_path, new_prefix, depth + 1, max_depth)\n",
    "\n",
    "# affiche la racine du projet (ajustez le chemin si nécessaire)\n",
    "print_tree('.', max_depth=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f280d7a",
   "metadata": {},
   "source": [
    "\n",
    "Un aperçu des dossiers importants :\n",
    "\n",
    "- **0_raw/** : contient les données brutes ou des fichiers vides (`.gitkeep`) servant de placeholder pour versionner le dossier.\n",
    "\n",
    "- **1_datasets/** : jeux de données préparés pour l’entraînement.\n",
    "  - **detection_trains/** : images et labels pour la détection des trains en format YOLO, divisés en `train/`, `val/` et `test/`.\n",
    "  - **segmentation_rails/** : images et labels pour la segmentation des voies (rails), divisés en `train/`, `val/` et `test/`.\n",
    "\n",
    "- **2_configs/** : fichiers de configuration YAML décrivant les jeux de données (chemins et noms de classes).  \n",
    "  - `yolo/data_trains.yaml` définit le jeu de données de détection avec une seule classe `train`.\n",
    "  - `yolo/data_rails.yaml` (ou `data_rails_1class.yaml`) définit le jeu de données de segmentation avec une classe `voie`.\n",
    "\n",
    "- **3_training/** : dossiers créés par les entraînements Ultralytics (`runs/detect/...`, `runs/segment/...`) et éventuellement des checkpoints.\n",
    "\n",
    "- **4_models/** : modèles entraînés et exportés. On y dépose les fichiers `best.pt` et `best.onnx` pour la détection et la segmentation.\n",
    "\n",
    "- **5_inference/** : scripts d’inférence et, le cas échéant, des échantillons.\n",
    "  - **scripts/** contient les scripts Python d’inférence : `infer_rails.py`, `infer_trains.py`, `infer_trains_and_rails.py` et `infer_trains_and_rails_with_history.py`.\n",
    "  - **samples/** peut contenir des exemples de vidéos d’entrée pour tester les scripts.\n",
    "\n",
    "- **6_evaluation/** : rapports et graphiques produits lors de l’évaluation des modèles.\n",
    "\n",
    "- **7_outputs/** : résultats des scripts d’inférence.  \n",
    "  - **7_outputs/overlays/** : vidéos annotées (fichiers MP4).  \n",
    "  - **7_outputs/predictions/** : fichiers JSONL et CSV contenant les prédictions et/ou l’historique d’occupation des voies pour chaque frame.\n",
    "\n",
    "- **8_inputs/** : peut contenir des fichiers d’entrée temporaires (par exemple des vidéos de test).  \n",
    "\n",
    "- **data_trains/** et **data_rails/** : jeux de données d’origine (images + labels) avant préparation.  \n",
    "  Ces dossiers servent de sources pour générer les jeux de données dans `1_datasets`.\n",
    "\n",
    "- **yolo11s.pt** et **yolo11s-seg.pt** : poids pré‑entraînés fournis par Ultralytics, utilisés comme point de départ pour l’entraînement des modèles de détection (YOLOv11) et de segmentation (YOLOv11‑seg).\n",
    "\n",
    "- **video.mp4**, **video2.mp4** : vidéos d’exemple pour tester les modèles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7b41f6",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Rôle de chaque dossier et fichiers clés {#Roles}\n",
    "\n",
    "Le dépôt suit une structure hiérarchique visant à organiser les données, la configuration, l’entraînement et l’exploitation des modèles.\n",
    "\n",
    "### Dossiers de données\n",
    "\n",
    "- **0_raw/** : réservé aux données brutes initiales. Un fichier `.gitkeep` y est présent pour maintenir le dossier dans le système de version.\n",
    "\n",
    "- **data_trains/** et **data_rails/** : dossiers d’images et de labels annotés initialement.  \n",
    "  * `data_trains/images/` et `data_trains/labels/` contiennent les images de trains et les fichiers d’annotations YOLO (format segmentation) pour la détection des trains.  \n",
    "  * `data_rails/images/` et `data_rails/labels/` contiennent les images et annotations (polygones) des voies pour la segmentation.  \n",
    "  Ces jeux ont été traités pour générer les sous‑jeux utilisables par YOLO.\n",
    "\n",
    "- **1_datasets/** : jeux de données préparés pour l’entraînement.\n",
    "  - **detection_trains/images/** et **detection_trains/labels/** : contiennent les images et labels destinés à l’entraînement (train), à la validation (val) et au test (test).  \n",
    "    Les labels ont été convertis du format segmentation (polygones) vers le format détection (boîtes englobantes) et toutes les classes ont été remappées sur un identifiant unique `0` (`train`).  \n",
    "    La répartition typique est 80 % pour l’entraînement, 10 % pour la validation et 10 % pour le test.\n",
    "  - **segmentation_rails/images/** et **segmentation_rails/labels/** : contiennent les images et masques pour la segmentation des voies.  \n",
    "    Toutes les classes de voies ont été remappées en une seule classe `0` (`voie`).\n",
    "\n",
    "### Dossiers de configuration\n",
    "\n",
    "- **2_configs/yolo/** et **2_configs/segmentation/** : YAML décrivant les jeux de données. Ces fichiers sont utilisés par Ultralytics pour charger les données et associer les indices de classe à des noms lisibles.  \n",
    "\n",
    "### Dossiers d’entraînement\n",
    "\n",
    "- **3_training/runs/** : dossiers générés automatiquement par Ultralytics lors de l’entraînement (`runs/detect/trainX/` et `runs/segment/trainY/`). On y trouve les logs, les courbes d’apprentissage et les fichiers `weights/best.pt` et `weights/last.pt` pour chaque entraînement.  \n",
    "- **3_training/checkpoints/** (facultatif) : peut contenir des copies sauvegardées des modèles à différentes étapes.\n",
    "\n",
    "### Dossiers de modèles\n",
    "\n",
    "- **4_models/detection/** : recopie finale des modèles de détection (poids en `.pt` et versions exportées en `.onnx` pour le backend).\n",
    "\n",
    "- **4_models/segmentation/** : recopie finale des modèles de segmentation.\n",
    "\n",
    "- **4_models/exports/** : versions exportées (`.onnx`, `.json`, etc.) prêtes pour l’inférence hors Python.\n",
    "\n",
    "### Dossiers d’inférence\n",
    "\n",
    "- **5_inference/scripts/** : scripts Python réalisant l’inférence sur des images ou des vidéos.  \n",
    "  Ces scripts chargent les modèles, appliquent la détection ou la segmentation, produisent des vidéos annotées et génèrent des fichiers JSON/CSV décrivant les prédictions et l’occupation des voies.\n",
    "\n",
    "- **5_inference/samples/** : exemples de vidéos d’entrée permettant de tester les scripts.\n",
    "\n",
    "### Dossiers d’évaluation et de sortie\n",
    "\n",
    "- **6_evaluation/** : rapports, métriques et figures générés lors de la validation des modèles.\n",
    "\n",
    "- **7_outputs/** : résultats des scripts d’inférence.  \n",
    "  - **7_outputs/overlays/** : vidéos annotées (fichiers MP4).  \n",
    "  - **7_outputs/predictions/** : fichiers JSONL et CSV contenant les prédictions et/ou l’historique d’occupation des voies pour chaque frame.\n",
    "\n",
    "### Fichiers utilitaires\n",
    "\n",
    "- **rename.py** : script utilitaire pour renommer des fichiers ou adapter la structure des labels (par exemple, conversion des polygones en boîtes).\n",
    "\n",
    "- **requirements.txt** : liste des dépendances Python nécessaires (par exemple `ultralytics`, `opencv-python`, `numpy`, etc.).  \n",
    "  Exécutez `pip install -r requirements.txt` pour installer l’environnement.\n",
    "\n",
    "- **yolo11s.pt** / **yolo11s-seg.pt** : poids pré‑entraînés de la famille Ultralytics YOLOv11 servant de base pour les entraînements de détection et de segmentation.\n",
    "\n",
    "- **video.mp4**, **video2.mp4** : vidéos d’exemple pour tester les modèles.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68474465",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Préparation des données et entraînement des modèles {#Entrainement}\n",
    "\n",
    "Cette section décrit la transformation des données brutes en jeux exploitables par les modèles et explique comment lancer l’entraînement.\n",
    "\n",
    "### Préparation des données\n",
    "\n",
    "1. **Récupération des données brutes** : les images annotées se trouvent dans `data_trains` et `data_rails`. Les fichiers `.txt` contiennent des polygones (YOLO‑segmentation).  \n",
    "2. **Conversion des labels** : pour la détection des trains, les polygones ont été convertis en boîtes englobantes et toutes les classes ont été fusionnées en une seule classe `train`. Les scripts `convert_seg_to_det.py` et `remap_labels_to_one_class.py` automatisent cette étape. Pour la segmentation des voies, toutes les classes `voie1..voie6` ont été remappées sur un identifiant unique `0` (`voie`).  \n",
    "3. **Répartition train/val/test** : les scripts `split_yolo_detection.py` et `split_yolo_seg_rails.py` créent la répartition 80/10/10 en copiant les fichiers dans `1_datasets/detection_trains` et `1_datasets/segmentation_rails`.\n",
    "4. **Création des YAML de configuration** : les fichiers `2_configs/yolo/data_trains.yaml` et `2_configs/yolo/data_rails_1class.yaml` décrivent les chemins (`path`, `train`, `val`, `test`) et les noms de classes. Exemple :\n",
    "\n",
    "```yaml\n",
    "# 2_configs/yolo/data_trains.yaml\n",
    "path: 1_datasets/detection_trains\n",
    "train: images/train\n",
    "val: images/val\n",
    "test: images/test\n",
    "names:\n",
    "  0: train\n",
    "```\n",
    "\n",
    "```yaml\n",
    "# 2_configs/yolo/data_rails_1class.yaml\n",
    "path: 1_datasets/segmentation_rails\n",
    "train: images/train\n",
    "val: images/val\n",
    "test: images/test\n",
    "names:\n",
    "  0: voie\n",
    "```\n",
    "\n",
    "Ces fichiers sont passés à Ultralytics pour charger les jeux.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac272a4",
   "metadata": {},
   "source": [
    "\n",
    "### 3.1 Entraînement du modèle de détection des trains {#Detection}\n",
    "\n",
    "1. **Installation de la bibliothèque Ultralytics** :\n",
    "\n",
    "```bash\n",
    "pip install ultralytics opencv-python numpy\n",
    "```\n",
    "\n",
    "2. **Lancement de l’entraînement** :\n",
    "\n",
    "Utilisez la commande `ultralytics detect train` (ou `yolo detect train` selon la version) pour entraîner le modèle sur les données préparées :\n",
    "\n",
    "```bash\n",
    "# depuis la racine du projet\n",
    "ultralytics detect train   model=yolo11s.pt   data=2_configs/yolo/data_trains.yaml   epochs=60   imgsz=640   batch=8   name=train_trains_1class\n",
    "```\n",
    "\n",
    "- **model** : poids pré‑entraînés servant de point de départ (`yolo11s.pt`).\n",
    "- **data** : chemin vers le fichier YAML décrivant les données.\n",
    "- **epochs** : nombre d’époques (60 dans notre cas). Le temps d’entraînement dépend du matériel (GPU recommandé).  \n",
    "- **imgsz** : taille des images (640×640).  \n",
    "- **batch** : taille du lot.  \n",
    "- **name** : nom du run ; un dossier `runs/detect/{name}/` sera créé.\n",
    "\n",
    "3. **Sorties de l’entraînement** : Ultralytics crée un dossier `runs/detect/<nom>/` avec :\n",
    "   - `weights/best.pt` et `weights/last.pt` : meilleurs poids et derniers poids.\n",
    "   - `results.png` : courbes de perte et de métriques.\n",
    "   - `confusion_matrix.png` : matrice de confusion.\n",
    "\n",
    "4. **Remarque sur les classes** : dans ce projet, toutes les classes `train1..train6` ont été fusionnées car la numérotation gauche‑droite est appliquée après l’inférence. Un apprentissage multi‑classes sur des rangs spatiaux n’est pas recommandé.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a31656c",
   "metadata": {},
   "source": [
    "\n",
    "### 3.2 Entraînement du modèle de segmentation des voies {#Segmentation}\n",
    "\n",
    "1. **Remappage des labels** : toutes les classes `voie1..voie6` ont été fusionnées en une seule classe `voie`. Le script `remap_rails_seg_to_one_class.py` réalise cette opération automatiquement.\n",
    "\n",
    "2. **Répartition train/val/test** : le script `split_yolo_seg_rails.py` copie les images et masques dans `1_datasets/segmentation_rails` avec la répartition souhaitée.\n",
    "\n",
    "3. **Entraînement YOLO segmentation** :\n",
    "\n",
    "```bash\n",
    "ultralytics segment train   model=yolo11s-seg.pt   data=2_configs/yolo/data_rails_1class.yaml   epochs=60   imgsz=640   batch=8   name=train_rails_1class\n",
    "```\n",
    "\n",
    "4. **Sorties de l’entraînement** : le dossier `runs/segment/<nom>/` contient les poids (`best.pt`, `last.pt`), les courbes de perte et des images de prédiction.\n",
    "\n",
    "5. **Exportation ONNX** :\n",
    "\n",
    "Pour intégrer le modèle dans un backend sans PyTorch, exportez le modèle en ONNX :\n",
    "\n",
    "```bash\n",
    "yolo export model=runs/segment/<nom>/weights/best.pt format=onnx opset=12\n",
    "```\n",
    "\n",
    "Le fichier `best.onnx` sera généré et placé dans le même dossier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d64efd",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Validation et tests {#Validation}\n",
    "\n",
    "Après l’entraînement, il est essentiel de mesurer la performance des modèles sur le jeu de test. Ultralytics propose des commandes `val` et `predict` :\n",
    "\n",
    "- **Évaluation quantitative** :\n",
    "\n",
    "```bash\n",
    "# Détection des trains\n",
    "ultralytics detect val   model=runs/detect/train_trains_1class/weights/best.pt   data=2_configs/yolo/data_trains.yaml   split=test\n",
    "\n",
    "# Segmentation des voies\n",
    "yolo segment val   model=runs/segment/train_rails_1class/weights/best.pt   data=2_configs/yolo/data_rails_1class.yaml   split=test\n",
    "```\n",
    "\n",
    "Les résultats affichent la précision, le rappel et les mAP (moyennes de la précision) et génèrent un dossier `runs/detect/val*` ou `runs/segment/val*` avec des images d’exemple.\n",
    "\n",
    "- **Évaluation qualitative** : visualiser les prédictions sur des images ou vidéos :\n",
    "\n",
    "```bash\n",
    "# Visualiser la détection des trains sur le jeu test\n",
    "ultralytics detect predict   model=runs/detect/train_trains_1class/weights/best.pt   source=1_datasets/detection_trains/images/test   save=True\n",
    "\n",
    "# Visualiser la segmentation des rails sur une vidéo\n",
    "yolo segment predict   model=runs/segment/train_rails_1class/weights/best.onnx   source=video.mp4   save=True\n",
    "```\n",
    "\n",
    "Des fichiers annotés seront sauvegardés dans `runs/detect/predict*` ou `runs/segment/predict*`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1df74c",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Scripts d’inférence {#Inference}\n",
    "\n",
    "Les scripts d’inférence fournis dans `5_inference/scripts/` permettent d’utiliser les modèles entraînés pour traiter des vidéos et produire des sorties structurées. Ils reposent sur les bibliothèques Ultralytics et OpenCV. Voici un résumé de leur fonctionnement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1d8684",
   "metadata": {},
   "source": [
    "\n",
    "### 5.1 Inférence des voies (rails) {#InfRails}\n",
    "\n",
    "Le script `infer_rails.py` charge un modèle de segmentation (`best.onnx`) et applique la segmentation à chaque frame d’une vidéo.  \n",
    "\n",
    "- **Entrées** :\n",
    "  - `MODEL_PATH` : chemin vers le modèle de segmentation (`runs/segment/train_rails_1class/weights/best.onnx`).\n",
    "  - `SOURCE_VIDEO` : chemin de la vidéo d’entrée (ex. `video.mp4`).\n",
    "  - Paramètres optionnels : taille d’image (`IMGSZ`), seuil de confiance (`CONF`), seuil du masque (`MASK_THRESH`).\n",
    "\n",
    "- **Traitement** :\n",
    "  1. Chargement du modèle via `YOLO(MODEL_PATH)`.\n",
    "  2. Parcours de la vidéo image par image avec OpenCV.\n",
    "  3. Inférence pour récupérer les masques prédits (`r.masks.data`), création d’un masque binaire global (`mask_bin`).\n",
    "  4. Détection des **rails** par composantes connexes : on applique `cv2.connectedComponentsWithStats` sur `mask_bin` pour identifier chaque rail.\n",
    "  5. Tri des rails de gauche à droite en fonction de la coordonnée `x` de leur centre. Chaque rail se voit attribuer un rang `voie1` à `voie6`.\n",
    "  6. Sauvegarde de la vidéo annotée dans `7_outputs/overlays/rails_overlay.mp4` et d’un fichier JSONL pour chaque frame dans `7_outputs/predictions/rails_per_frame.jsonl`.\n",
    "\n",
    "- **Sorties** :\n",
    "  - Vidéo annotée (overlay) : rails colorés en vert et numérotés.\n",
    "  - Fichier JSONL par frame : indique combien de rails ont été détectés, leurs coordonnées et l’ordre gauche→droite.  \n",
    "\n",
    "Par exemple :\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"frame\": 42,\n",
    "  \"rails_detected\": 6,\n",
    "  \"rails\": [\n",
    "    {\"rank\": 1, \"label\": \"voie1\", \"bbox\": [x1, y1, x2, y2], \"cx\": 120.3, \"area\": 50456},\n",
    "    …\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "Ce script facilite l’intégration de la segmentation dans un backend sans nécessiter PyTorch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477741a2",
   "metadata": {},
   "source": [
    "\n",
    "### 5.2 Inférence des trains {#InfTrains}\n",
    "\n",
    "Le script `infer_trains.py` réalise la détection et le suivi des trains à partir d’un modèle de détection YOLOv11 (`best.pt`).  \n",
    "Il utilise l’algorithme **BoT‑SORT** pour attribuer un identifiant (`track_id`) à chaque train et garantir la persistance d’un train d’une frame à l’autre.\n",
    "\n",
    "- **Entrées** :\n",
    "  - `MODEL_PATH` : chemin vers le modèle de détection (`runs/detect/train_trains_1class/weights/best.pt`).\n",
    "  - `SOURCE_VIDEO` : vidéo d’entrée.\n",
    "  - Paramètres : taille d’image (`IMGSZ`), seuil de confiance (`CONF`), seuil d’IoU (`IOU`), chemin vers le fichier de configuration du tracker (`botsort.yaml`).\n",
    "\n",
    "- **Traitement** :\n",
    "  1. Chargement du modèle via `YOLO(MODEL_PATH)`.\n",
    "  2. Parcours de la vidéo et appel à `model.track()` pour obtenir les boîtes (`boxes.xyxy`), les scores (`boxes.conf`), les classes (`boxes.cls`) et les identifiants de suivi (`boxes.id`).\n",
    "  3. Calcul du centre X des boîtes et tri de gauche à droite pour attribuer un rang (`train1` à `train6`). Ce rang est indépendant de la classe car toutes les classes ont été fusionnées.\n",
    "  4. Sauvegarde d’une vidéo annotée (`trains_track_overlay.mp4`) et d’un fichier JSONL (`trains_per_frame.jsonl`) contenant pour chaque frame les boîtes, les scores, les identifiants de suivi et les rangs gauche→droite.\n",
    "\n",
    "- **Sorties** :\n",
    "  - Vidéo overlay montrant les boîtes des trains, leur identifiant et leur rang gauche→droite.\n",
    "  - Fichier JSONL listant les trains et leurs propriétés pour chaque frame.\n",
    "\n",
    "Ce script ne gère pas les rails : il se concentre uniquement sur la détection et le tracking des trains.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e04ca69",
   "metadata": {},
   "source": [
    "\n",
    "### 5.3 Script combiné trains + rails {#InfCombi}\n",
    "\n",
    "Le script `infer_trains_and_rails_with_history.py` combine la détection des trains et la segmentation des rails pour fournir une compréhension complète de la scène.  \n",
    "Il associe chaque train détecté à la voie sur laquelle il se trouve et construit un historique d’occupation.\n",
    "\n",
    "- **Entrées** :\n",
    "  - `TRAINS_MODEL` : modèle de détection des trains (`best.pt`).\n",
    "  - `RAILS_MODEL` : modèle de segmentation des rails (`best.onnx`).\n",
    "  - `SOURCE_VIDEO` : vidéo à analyser.\n",
    "\n",
    "- **Étapes principales** :\n",
    "\n",
    "  1. **Segmentation des rails** : on applique le modèle de segmentation sur chaque frame pour obtenir un masque binaire. Les composantes connexes de ce masque sont analysées pour identifier jusqu’à six rails, triés de gauche à droite.\n",
    "\n",
    "  2. **Détection des trains + tracking** : on détecte les trains via `model.track()` et on récupère les identifiants de suivi (`track_id`). Les trains sont triés de gauche à droite afin de leur attribuer les labels `train1..train6`.\n",
    "\n",
    "  3. **Association train→voie** : pour chaque train, on prend le point bas‑centre de sa boîte englobante et on regarde dans la matrice de composantes pour déterminer sur quelle voie il se trouve.  \n",
    "     Si le point tombe en dehors du masque, aucune voie n’est attribuée (`None`).\n",
    "\n",
    "  4. **Historique d’occupation** : le script garde en mémoire l’ancienne voie associée à chaque train et génère un événement lorsque le train change de voie.  \n",
    "     Cela permet de produire des statistiques comme la durée passée sur chaque voie et d’exporter un historique en fin de traitement.\n",
    "\n",
    "- **Sorties** :\n",
    "\n",
    "  - **Vidéo annotée** (`trains_rails_overlay.mp4`) : chaque rail est coloré en vert et numéroté, chaque train est entouré d’une boîte colorée avec son identifiant, son rang (`train3` par exemple) et la voie sur laquelle il se trouve (`voie2`).  \n",
    "  - **Fichier `trains_rails_per_frame.jsonl`** : pour chaque frame, la liste des rails détectés et des trains avec leur voie associée et leur rang gauche→droite.  \n",
    "  - **Fichier `occupancy_per_frame.csv`** : un enregistrement tabulaire précisant, pour chaque frame, si une voie est occupée et par quels `track_id`.  \n",
    "  - **Fichier `occupancy_events.csv` et `occupancy_events.jsonl`** : un historique des événements, c’est‑à‑dire les moments où un train entre ou quitte une voie, avec les timestamps et la durée passée sur la voie précédente.  \n",
    "\n",
    "Ce script constitue la base d’un système de supervision : il fournit une vision temps réel de l’occupation des voies et une traçabilité historique.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850bc319",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Export de l’historique d’occupation {#Historique}\n",
    "\n",
    "L’historique d’occupation est utile pour analyser la durée de stationnement des trains sur chaque voie et détecter d’éventuels conflits (deux trains sur la même voie).  \n",
    "Le script combiné génère :\n",
    "\n",
    "- **occupancy_per_frame.csv** : fichier avec une ligne par voie et par frame. Il contient l’horodatage (`time_s`), le nom de la voie, un indicateur `occupied` (0/1) et la liste des `track_id` des trains présents.  \n",
    "\n",
    "- **occupancy_events.csv** : fichier listant les transitions de voie pour chaque train. Pour chaque événement, on a :\n",
    "  - l’identifiant du train (`track_id`),\n",
    "  - la voie quittée (`from_voie`),\n",
    "  - la voie rejointe (`to_voie`),\n",
    "  - la frame et l’horodatage de début et de fin,\n",
    "  - la durée passée sur la voie précédente.\n",
    "\n",
    "Ces fichiers peuvent être analysés dans un tableur ou un script Python pour extraire des statistiques (par exemple la voie la plus utilisée ou le temps d’occupation moyen par train).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ca57d9",
   "metadata": {},
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "Ce notebook a présenté la structure du dépôt Smart Yard, le rôle des dossiers et fichiers, les étapes de préparation des données, l’entraînement des modèles de détection et de segmentation, la validation, ainsi que les scripts d’inférence et l’export de l’historique d’occupation.  \n",
    "En suivant ces indications, un nouveau développeur ou un évaluateur peut reproduire l’ensemble du pipeline, du traitement des données brutes jusqu’à l’exploitation des modèles en production.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
